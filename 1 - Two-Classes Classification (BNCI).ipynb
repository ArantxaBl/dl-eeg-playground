{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 15, 2560)\n",
      "(640,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Format dataset, we read the file for the desired subject, and parse the data to extract:\n",
    "- samplingRate\n",
    "- trialLength\n",
    "- X, a M x N x K matrix, which stands for trial x chan x samples\n",
    "                         the actual values are 160 x 15 x 2560\n",
    "- y, a M vector containing the labels {0,1}\n",
    "\n",
    "ref:\n",
    "Dataset description: https://lampx.tugraz.at/~bci/database/002-2014/description.pdf\n",
    "\"\"\"\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# prepare data containers\n",
    "y = []\n",
    "X = []\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "trainingFileList = ['BBCIData/S14T.mat', \n",
    "                    'BBCIData/S13T.mat', \n",
    "                    'BBCIData/S12T.mat', \n",
    "                    'BBCIData/S11T.mat', \n",
    "                    'BBCIData/S10T.mat', \n",
    "                    'BBCIData/S09T.mat', \n",
    "                    'BBCIData/S08T.mat', \n",
    "                    'BBCIData/S07T.mat', \n",
    "                    'BBCIData/S06T.mat', \n",
    "                    'BBCIData/S05T.mat', \n",
    "                    'BBCIData/S04T.mat', \n",
    "                    'BBCIData/S03T.mat', \n",
    "                    'BBCIData/S02T.mat', \n",
    "                    'BBCIData/S01T.mat']\n",
    "\n",
    "validationFileList = ['BBCIData/S14E.mat', \n",
    "                      'BBCIData/S13E.mat', \n",
    "                      'BBCIData/S12E.mat', \n",
    "                      'BBCIData/S11E.mat', \n",
    "                      'BBCIData/S10E.mat', \n",
    "                      'BBCIData/S09E.mat', \n",
    "                      'BBCIData/S08E.mat', \n",
    "                      'BBCIData/S07E.mat', \n",
    "                      'BBCIData/S06E.mat', \n",
    "                      'BBCIData/S05E.mat', \n",
    "                      'BBCIData/S04E.mat', \n",
    "                      'BBCIData/S03E.mat', \n",
    "                      'BBCIData/S02E.mat', \n",
    "                      'BBCIData/S01E.mat']\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "trainingFileList = ['BBCIData/S04T.mat',\n",
    "                    'BBCIData/S03T.mat', \n",
    "                    'BBCIData/S02T.mat', \n",
    "                    'BBCIData/S01T.mat']\n",
    "\n",
    "validationFileList = ['BBCIData/S04E.mat',\n",
    "                      'BBCIData/S03E.mat', \n",
    "                      'BBCIData/S02E.mat', \n",
    "                      'BBCIData/S01E.mat']\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(trainingFileList)):\n",
    "    # read file\n",
    "    d1T = sio.loadmat(trainingFileList[i])\n",
    "    d1E = sio.loadmat(validationFileList[i])\n",
    "    \n",
    "    samplingRate = d1T['data'][0][0][0][0][3][0][0]\n",
    "    trialLength = 5*samplingRate\n",
    "\n",
    "\n",
    "    # run through all training runs\n",
    "    for run in range(5):\n",
    "        y.append(d1T['data'][0][run][0][0][2][0]) # labels\n",
    "        timestamps = d1T['data'][0][run][0][0][1][0] # timestamps\n",
    "        rawData = d1T['data'][0][run][0][0][0].transpose() # chan x data\n",
    "\n",
    "        # parse out data based on timestamps\n",
    "        for start in timestamps:\n",
    "            end = start + trialLength\n",
    "            X.append(rawData[:,start:end]) #15 x 2560\n",
    "\n",
    "\n",
    "    # run through all validation runs (we do not discriminate at this point)\n",
    "    for run in range(3):\n",
    "        y.append(d1E['data'][0][run][0][0][2][0]) # labels\n",
    "        timestamps = d1E['data'][0][run][0][0][1][0] # timestamps\n",
    "        rawData = d1E['data'][0][run][0][0][0].transpose() # chan x data\n",
    "\n",
    "        # parse out data based on timestamps\n",
    "        for start in timestamps:\n",
    "            end = start + trialLength\n",
    "            X.append(rawData[:,start:end]) #15 x 2560\n",
    "\n",
    "# arrange data into numpy arrays\n",
    "# also torch expect float32 for samples\n",
    "# and int64 for labels {0,1}\n",
    "X = np.array(X).astype(np.float32)\n",
    "y = (np.array(y).flatten()-1).astype(np.int64)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# erase unused references\n",
    "d1T = []\n",
    "d1E = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil.signal_target import SignalAndTarget\n",
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from torch import nn\n",
    "from braindecode.torch_ext.util import set_random_seeds    \n",
    "from torch import optim\n",
    "\n",
    "\n",
    "idx = np.random.permutation(X.shape[0])\n",
    "\n",
    "X = X[idx,:,:]\n",
    "y = y[idx]\n",
    "\n",
    "#print(X.shape)\n",
    "#print(y.shape)\n",
    "\n",
    "nb_train_trials = int(np.floor(5/8*X.shape[0]))\n",
    "\n",
    "\n",
    "train_set = SignalAndTarget(X[:nb_train_trials], y=y[:nb_train_trials])\n",
    "test_set = SignalAndTarget(X[nb_train_trials:], y=y[nb_train_trials:])\n",
    "\n",
    "#train_set = SignalAndTarget(X[:nb_train_trials], y=y[:nb_train_trials])\n",
    "#test_set = SignalAndTarget(X[nb_train_trials:nb_test_trials], y=y[nb_train_trials:nb_test_trials])\n",
    "\n",
    "# Set if you want to use GPU\n",
    "# You can also use torch.cuda.is_available() to determine if cuda is available on your machine.\n",
    "cuda = False\n",
    "set_random_seeds(seed=20170629, cuda=cuda)\n",
    "n_classes = 2\n",
    "in_chans = train_set.X.shape[1]\n",
    "# final_conv_length = auto ensures we only get a single output in the time dimension\n",
    "model = ShallowFBCSPNet(in_chans=in_chans, n_classes=n_classes,\n",
    "                        input_time_length=train_set.X.shape[2],\n",
    "                        final_conv_length='auto').create_network()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train  Loss: 0.67308\n",
      "Train  Accuracy: 66.2%\n",
      "Test   Loss: 1.09870\n",
      "Test   Accuracy: 46.7%\n",
      "Epoch 1\n",
      "Train  Loss: 0.63049\n",
      "Train  Accuracy: 67.0%\n",
      "Test   Loss: 1.06700\n",
      "Test   Accuracy: 47.9%\n",
      "Epoch 2\n",
      "Train  Loss: 0.57933\n",
      "Train  Accuracy: 71.8%\n",
      "Test   Loss: 1.06950\n",
      "Test   Accuracy: 49.2%\n",
      "Epoch 3\n",
      "Train  Loss: 0.45585\n",
      "Train  Accuracy: 76.8%\n",
      "Test   Loss: 0.98647\n",
      "Test   Accuracy: 50.4%\n",
      "Epoch 4\n",
      "Train  Loss: 0.39514\n",
      "Train  Accuracy: 82.8%\n",
      "Test   Loss: 0.98505\n",
      "Test   Accuracy: 53.8%\n",
      "Epoch 5\n",
      "Train  Loss: 0.41111\n",
      "Train  Accuracy: 81.2%\n",
      "Test   Loss: 0.99206\n",
      "Test   Accuracy: 52.9%\n",
      "Epoch 6\n",
      "Train  Loss: 0.47935\n",
      "Train  Accuracy: 79.0%\n",
      "Test   Loss: 1.15841\n",
      "Test   Accuracy: 52.9%\n",
      "Epoch 7\n",
      "Train  Loss: 0.44434\n",
      "Train  Accuracy: 83.5%\n",
      "Test   Loss: 1.16125\n",
      "Test   Accuracy: 52.5%\n",
      "Epoch 8\n",
      "Train  Loss: 0.23925\n",
      "Train  Accuracy: 91.0%\n",
      "Test   Loss: 0.94978\n",
      "Test   Accuracy: 57.9%\n",
      "Epoch 9\n",
      "Train  Loss: 0.24268\n",
      "Train  Accuracy: 91.5%\n",
      "Test   Loss: 0.91686\n",
      "Test   Accuracy: 57.9%\n",
      "Epoch 10\n",
      "Train  Loss: 0.18259\n",
      "Train  Accuracy: 94.5%\n",
      "Test   Loss: 0.92877\n",
      "Test   Accuracy: 58.8%\n",
      "Epoch 11\n",
      "Train  Loss: 0.21589\n",
      "Train  Accuracy: 92.8%\n",
      "Test   Loss: 1.10934\n",
      "Test   Accuracy: 50.8%\n",
      "Epoch 12\n",
      "Train  Loss: 0.19750\n",
      "Train  Accuracy: 92.5%\n",
      "Test   Loss: 0.99717\n",
      "Test   Accuracy: 61.7%\n",
      "Epoch 13\n",
      "Train  Loss: 0.16071\n",
      "Train  Accuracy: 95.5%\n",
      "Test   Loss: 1.02683\n",
      "Test   Accuracy: 58.8%\n",
      "Epoch 14\n",
      "Train  Loss: 0.22487\n",
      "Train  Accuracy: 91.5%\n",
      "Test   Loss: 1.12432\n",
      "Test   Accuracy: 62.5%\n",
      "Epoch 15\n",
      "Train  Loss: 0.12301\n",
      "Train  Accuracy: 95.8%\n",
      "Test   Loss: 1.00252\n",
      "Test   Accuracy: 60.4%\n",
      "Epoch 16\n",
      "Train  Loss: 0.10149\n",
      "Train  Accuracy: 97.2%\n",
      "Test   Loss: 1.05095\n",
      "Test   Accuracy: 60.4%\n",
      "Epoch 17\n",
      "Train  Loss: 0.07680\n",
      "Train  Accuracy: 98.5%\n",
      "Test   Loss: 0.93779\n",
      "Test   Accuracy: 63.3%\n",
      "Epoch 18\n",
      "Train  Loss: 0.10049\n",
      "Train  Accuracy: 97.0%\n",
      "Test   Loss: 1.08101\n",
      "Test   Accuracy: 60.8%\n",
      "Epoch 19\n",
      "Train  Loss: 0.06873\n",
      "Train  Accuracy: 99.2%\n",
      "Test   Loss: 1.05725\n",
      "Test   Accuracy: 60.0%\n",
      "Epoch 20\n",
      "Train  Loss: 0.08276\n",
      "Train  Accuracy: 97.5%\n",
      "Test   Loss: 0.99826\n",
      "Test   Accuracy: 65.8%\n",
      "Epoch 21\n",
      "Train  Loss: 0.10955\n",
      "Train  Accuracy: 96.0%\n",
      "Test   Loss: 1.21967\n",
      "Test   Accuracy: 59.6%\n",
      "Epoch 22\n",
      "Train  Loss: 0.06698\n",
      "Train  Accuracy: 98.8%\n",
      "Test   Loss: 1.12648\n",
      "Test   Accuracy: 60.4%\n",
      "Epoch 23\n",
      "Train  Loss: 0.05459\n",
      "Train  Accuracy: 98.8%\n",
      "Test   Loss: 1.18137\n",
      "Test   Accuracy: 58.8%\n",
      "Epoch 24\n",
      "Train  Loss: 0.04781\n",
      "Train  Accuracy: 98.5%\n",
      "Test   Loss: 1.00253\n",
      "Test   Accuracy: 63.3%\n",
      "Epoch 25\n",
      "Train  Loss: 0.03735\n",
      "Train  Accuracy: 100.0%\n",
      "Test   Loss: 1.14408\n",
      "Test   Accuracy: 57.9%\n",
      "Epoch 26\n",
      "Train  Loss: 0.10257\n",
      "Train  Accuracy: 96.0%\n",
      "Test   Loss: 1.38383\n",
      "Test   Accuracy: 56.2%\n",
      "Epoch 27\n",
      "Train  Loss: 0.03526\n",
      "Train  Accuracy: 100.0%\n",
      "Test   Loss: 1.14408\n",
      "Test   Accuracy: 58.8%\n",
      "Epoch 28\n",
      "Train  Loss: 0.04067\n",
      "Train  Accuracy: 99.5%\n",
      "Test   Loss: 1.20518\n",
      "Test   Accuracy: 60.0%\n",
      "Epoch 29\n",
      "Train  Loss: 0.04384\n",
      "Train  Accuracy: 99.2%\n",
      "Test   Loss: 1.21764\n",
      "Test   Accuracy: 59.6%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from braindecode.datautil.iterators import get_balanced_batches\n",
    "import torch.nn.functional as F\n",
    "from numpy.random import RandomState\n",
    "rng = RandomState((2017,6,30))\n",
    "for i_epoch in range(30):\n",
    "    i_trials_in_batch = get_balanced_batches(len(train_set.X), rng, shuffle=True,\n",
    "                                            batch_size=15)\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    for i_trials in i_trials_in_batch:\n",
    "        # Have to add empty fourth dimension to X\n",
    "        batch_X = train_set.X[i_trials][:,:,:,None]\n",
    "        batch_y = train_set.y[i_trials]\n",
    "        net_in = np_to_var(batch_X)\n",
    "        if cuda:\n",
    "            net_in = net_in.cuda()\n",
    "        net_target = np_to_var(batch_y)\n",
    "        if cuda:\n",
    "            net_target = net_target.cuda()\n",
    "        # Remove gradients of last backward pass from all parameters\n",
    "        optimizer.zero_grad()\n",
    "        # Compute outputs of the network\n",
    "        outputs = model(net_in)\n",
    "        # Compute the loss\n",
    "        loss = F.nll_loss(outputs, net_target)\n",
    "        # Do the backpropagation\n",
    "        loss.backward()\n",
    "        # Update parameters with the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print some statistics each epoch\n",
    "    model.eval()\n",
    "    print(\"Epoch {:d}\".format(i_epoch))\n",
    "    for setname, dataset in (('Train', train_set), ('Test', test_set)):\n",
    "        # Here, we will use the entire dataset at once, which is still possible\n",
    "        # for such smaller datasets. Otherwise we would have to use batches.\n",
    "        net_in = np_to_var(dataset.X[:,:,:,None])\n",
    "        if cuda:\n",
    "            net_in = net_in.cuda()\n",
    "        net_target = np_to_var(dataset.y)\n",
    "        if cuda:\n",
    "            net_target = net_target.cuda()\n",
    "        outputs = model(net_in)\n",
    "        loss = F.nll_loss(outputs, net_target)\n",
    "        print(\"{:6s} Loss: {:.5f}\".format(\n",
    "            setname, float(var_to_np(loss))))\n",
    "        predicted_labels = np.argmax(var_to_np(outputs), axis=1)\n",
    "        accuracy = np.mean(dataset.y  == predicted_labels)\n",
    "        print(\"{:6s} Accuracy: {:.1f}%\".format(\n",
    "            setname, accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: RAM not big enough\n",
    "# next session, manage batches through the hard drive\n",
    "# add analytics on training performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
