{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Format dataset, we read the file for the desired subject, and parse the data to extract:\n",
    "- samplingRate\n",
    "- trialLength\n",
    "- X, a M x N x K matrix, which stands for trial x chan x samples\n",
    "                         the actual values are 160 x 15 x 2560\n",
    "- y, a M vector containing the labels {0,1}\n",
    "\n",
    "ref:\n",
    "Dataset description: https://lampx.tugraz.at/~bci/database/002-2014/description.pdf\n",
    "\"\"\"\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# prepare data containers\n",
    "y = []\n",
    "X = []\n",
    "\n",
    "\n",
    "trainingFileList = ['BBCIData/S14T.mat', \n",
    "                    'BBCIData/S13T.mat', \n",
    "                    'BBCIData/S12T.mat', \n",
    "                    'BBCIData/S11T.mat', \n",
    "                    'BBCIData/S10T.mat', \n",
    "                    'BBCIData/S09T.mat', \n",
    "                    'BBCIData/S08T.mat', \n",
    "                    'BBCIData/S07T.mat', \n",
    "                    'BBCIData/S06T.mat', \n",
    "                    'BBCIData/S05T.mat', \n",
    "                    'BBCIData/S04T.mat', \n",
    "                    'BBCIData/S03T.mat', \n",
    "                    'BBCIData/S02T.mat', \n",
    "                    'BBCIData/S01T.mat']\n",
    "\n",
    "validationFileList = ['BBCIData/S14E.mat', \n",
    "                      'BBCIData/S13E.mat', \n",
    "                      'BBCIData/S12E.mat', \n",
    "                      'BBCIData/S11E.mat', \n",
    "                      'BBCIData/S10E.mat', \n",
    "                      'BBCIData/S09E.mat', \n",
    "                      'BBCIData/S08E.mat', \n",
    "                      'BBCIData/S07E.mat', \n",
    "                      'BBCIData/S06E.mat', \n",
    "                      'BBCIData/S05E.mat', \n",
    "                      'BBCIData/S04E.mat', \n",
    "                      'BBCIData/S03E.mat', \n",
    "                      'BBCIData/S02E.mat', \n",
    "                      'BBCIData/S01E.mat']\n",
    "\n",
    "\n",
    "for i in range(len(trainingFileList)):\n",
    "    # read file\n",
    "    d1T = sio.loadmat(trainingFileList[i])\n",
    "    d1E = sio.loadmat(validationFileList[i])\n",
    "    \n",
    "    samplingRate = d1T['data'][0][0][0][0][3][0][0]\n",
    "    trialLength = 5*samplingRate\n",
    "\n",
    "\n",
    "    # run through all training runs\n",
    "    for run in range(5):\n",
    "        y.append(d1T['data'][0][run][0][0][2][0]) # labels\n",
    "        timestamps = d1T['data'][0][run][0][0][1][0] # timestamps\n",
    "        rawData = d1T['data'][0][run][0][0][0].transpose() # chan x data\n",
    "\n",
    "        # parse out data based on timestamps\n",
    "        for start in timestamps:\n",
    "            end = start + trialLength\n",
    "            X.append(rawData[:,start:end]) #15 x 2560\n",
    "\n",
    "\n",
    "    # run through all validation runs (we do not discriminate at this point)\n",
    "    for run in range(3):\n",
    "        y.append(d1E['data'][0][run][0][0][2][0]) # labels\n",
    "        timestamps = d1E['data'][0][run][0][0][1][0] # timestamps\n",
    "        rawData = d1E['data'][0][run][0][0][0].transpose() # chan x data\n",
    "\n",
    "        # parse out data based on timestamps\n",
    "        for start in timestamps:\n",
    "            end = start + trialLength\n",
    "            X.append(rawData[:,start:end]) #15 x 2560\n",
    "\n",
    "    del rawData\n",
    "    del d1T\n",
    "    del d1E\n",
    "\n",
    "# arrange data into numpy arrays\n",
    "# also torch expect float32 for samples\n",
    "# and int64 for labels {0,1}\n",
    "X = np.array(X).astype(np.float32)\n",
    "y = (np.array(y).flatten()-1).astype(np.int64)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# erase unused references\n",
    "d1T = []\n",
    "d1E = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil.signal_target import SignalAndTarget\n",
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from torch import nn\n",
    "from braindecode.torch_ext.util import set_random_seeds    \n",
    "from torch import optim\n",
    "import torch\n",
    "\n",
    "idx = np.random.permutation(X.shape[0])\n",
    "\n",
    "X = X[idx,:,:]\n",
    "y = y[idx]\n",
    "\n",
    "#print(X.shape)\n",
    "#print(y.shape)\n",
    "\n",
    "nb_train_trials = int(np.floor(5/8*X.shape[0]))\n",
    "\n",
    "\n",
    "train_set = SignalAndTarget(X[:nb_train_trials], y=y[:nb_train_trials])\n",
    "test_set = SignalAndTarget(X[nb_train_trials:], y=y[nb_train_trials:])\n",
    "\n",
    "#train_set = SignalAndTarget(X[:nb_train_trials], y=y[:nb_train_trials])\n",
    "#test_set = SignalAndTarget(X[nb_train_trials:nb_test_trials], y=y[nb_train_trials:nb_test_trials])\n",
    "\n",
    "# Set if you want to use GPU\n",
    "# You can also use torch.cuda.is_available() to determine if cuda is available on your machine.\n",
    "cuda = torch.cuda.is_available()\n",
    "set_random_seeds(seed=20170629, cuda=cuda)\n",
    "n_classes = 2\n",
    "in_chans = train_set.X.shape[1]\n",
    "# final_conv_length = auto ensures we only get a single output in the time dimension\n",
    "model = ShallowFBCSPNet(in_chans=in_chans, n_classes=n_classes,\n",
    "                        input_time_length=train_set.X.shape[2],\n",
    "                        final_conv_length='auto').create_network()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "from braindecode.datautil.iterators import get_balanced_batches\n",
    "import torch.nn.functional as F\n",
    "from numpy.random import RandomState\n",
    "rng = RandomState((2017,6,30))\n",
    "for i_epoch in range(50):\n",
    "    i_trials_in_batch = get_balanced_batches(len(train_set.X), rng, shuffle=True,\n",
    "                                            batch_size=32)\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    for i_trials in i_trials_in_batch:\n",
    "        # Have to add empty fourth dimension to X\n",
    "        batch_X = train_set.X[i_trials][:,:,:,None]\n",
    "        batch_y = train_set.y[i_trials]\n",
    "        net_in = np_to_var(batch_X)\n",
    "        if cuda:\n",
    "            net_in = net_in.cuda()\n",
    "        net_target = np_to_var(batch_y)\n",
    "        if cuda:\n",
    "            net_target = net_target.cuda()\n",
    "        # Remove gradients of last backward pass from all parameters\n",
    "        optimizer.zero_grad()\n",
    "        # Compute outputs of the network\n",
    "        outputs = model(net_in)\n",
    "        # Compute the loss\n",
    "        loss = F.nll_loss(outputs, net_target)\n",
    "        # Do the backpropagation\n",
    "        loss.backward()\n",
    "        # Update parameters with the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print some statistics each epoch\n",
    "    model.eval()\n",
    "    print(\"Epoch {:d}\".format(i_epoch))\n",
    "    for setname, dataset in (('Train', train_set), ('Test', test_set)):\n",
    "        i_trials_in_batch = get_balanced_batches(len(dataset.X), rng, batch_size=32, shuffle=False)\n",
    "        outputs = []\n",
    "        net_targets = []\n",
    "        for i_trials in i_trials_in_batch:\n",
    "            batch_X = train_set.X[i_trials][:,:,:,None]\n",
    "            batch_y = train_set.y[i_trials]\n",
    "            \n",
    "            net_in = np_to_var(batch_X)\n",
    "            if cuda:\n",
    "                net_in = net_in.cuda()\n",
    "            net_target = np_to_var(batch_y)\n",
    "            if cuda:\n",
    "                net_target = net_target.cuda()\n",
    "            net_target = var_to_np(net_target)\n",
    "            output = var_to_np(model(net_in))\n",
    "            outputs.append(output)\n",
    "            net_targets.append(net_target)\n",
    "        net_targets = np_to_var(np.concatenate(net_targets))\n",
    "        outputs = np_to_var(np.concatenate(outputs))\n",
    "        loss = F.nll_loss(outputs, net_targets)\n",
    "        print(\"{:6s} Loss: {:.5f}\".format(\n",
    "            setname, float(var_to_np(loss))))\n",
    "        predicted_labels = np.argmax(var_to_np(outputs), axis=1)\n",
    "        accuracy = np.mean(dataset.y  == predicted_labels)\n",
    "        print(\"{:6s} Accuracy: {:.1f}%\".format(\n",
    "            setname, accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: RAM not big enough\n",
    "# next session, manage batches through the hard drive\n",
    "# add analytics on training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
